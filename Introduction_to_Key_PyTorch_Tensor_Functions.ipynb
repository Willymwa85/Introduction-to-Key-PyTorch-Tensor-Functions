{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Willymwa85/Introduction-to-Key-PyTorch-Tensor-Functions/blob/main/Introduction_to_Key_PyTorch_Tensor_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vyfKbXrgHEQh"
      },
      "outputs": [],
      "source": [
        "# Jovian Commit Essentials\n",
        "# Please retain and execute this cell without modifying the contents for 'jovian.commit' to work\n",
        "!pip install jovian --upgrade -q\n",
        "import jovian\n",
        "jovian.utils.colab.set_colab_file_id('1Yvg09oHywG6ckQi8WRySG0nWjppvU1KA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "167Vi_5TGeOX"
      },
      "source": [
        "# PyTorch Functions\n",
        "## Introduction\n",
        "PyTorch is an open source machine learning library based on Python and developed by Facebook's AI research group. It is used for applications such as computer vision and natural language processing.\n",
        "\n",
        "Some key features of PyTorch:\n",
        "\n",
        "- Provides tensors and dynamic neural networks with strong GPU acceleration. Tensors are similar to NumPy arrays but can also be run on GPUs.\n",
        "- Has an autograd system that automatically calculates gradients required for backpropagation in neural networks. This makes PyTorch a useful tool for deep learning research.\n",
        "- Supports neural network modules like convolution layers, recurrent layers, and linear layers. These can be used to build and train deep neural networks.\n",
        "- Integrates well with Python data science stacks like NumPy, SciPy and matplotlib.\n",
        "- Can be used for rapid prototyping and experimentation as model architectures can be defined and tweaked dynamically.\n",
        "- Has high performance distributed training capabilities via integration with libraries like Horovod.\n",
        "- Supported by companies like Facebook, Twitter and Nvidia. Has an active open source community contributing to it.\n",
        "\n",
        "In summary, PyTorch is a Python-based framework designed for building and training deep neural networks. Its key advantages are ease of use, flexibility, and speed, making it a popular choice for academics, researchers and companies to implement deep learning algorithms.\n",
        "\n",
        "The below 5 functions are the chosen functions for discussion.\n",
        "\n",
        "- torch.zeros_like\n",
        "- torch.normal\n",
        "- torch.hstack\n",
        "- torch.sqrt\n",
        "- torch.gather\n",
        "\n",
        "Before we begin, let's install and import PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fYnK2ZeYGeOZ"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "\n",
        "# Linux / Binder\n",
        "# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Windows\n",
        "# !pip install numpy torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS\n",
        "# !pip install numpy torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "t04E0fzPGeOc"
      },
      "outputs": [],
      "source": [
        "# Import torch and other required modules\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INNONdZ_GeOe"
      },
      "source": [
        "## Function 1 - torch.zeros_like\n",
        "PyTorch's torch.zeros_like() is a simple yet powerful function for initializing tensors. It creates a new tensor with the same shape as a given tensor and fills it with zeros.\n",
        "\n",
        "The key advantage of zeros_like() is convenience - it allows initializing tensors to zero with minimal coding. Just pass any existing tensor, and a zero-filled copy will be returned. This avoids manually specifying shapes, devices etc. when the desired structure is already available. It also helps ensure numeric stability when input values need to be reset before model calculations.\n",
        "\n",
        "Under the hood, zeros_like() replicates both the shape and device of the input tensor, ensuring compatibility with subsequent operations. However, it does not copy gradient information by default. The downside is that zeros_like() can allocate new memory, so may have performance costs vs in-place zeroing in some cases.\n",
        "\n",
        "Overall, torch.zeros_like() provides an easy and intuitive way to clear tensor values and prepare for future computations. The simplicity and brevity make it very useful across a wide range of deep learning use cases. It's an excellent example of PyTorch's design philosophy of fluent interfaces that minimize coding overhead for researchers and practitioners.\n",
        "- torch.zeros_like(input, dtype=None, layout=None, device=None, requires_grad=False)\n",
        "- Creates a tensor of zeros with the same size as the input tensor.\n",
        "- Usage: Initialize a tensor to zero for numeric stability in some models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ0v-RzQGeOg",
        "outputId": "b7aa3ec7-0dc9-4e56-d17e-2a33d065378f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# Example 1 - working\n",
        "x = torch.rand(2, 3)\n",
        "y = torch.zeros_like(x) # y is 2x3 tensor of zeros\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxgnYmIvGeOj"
      },
      "source": [
        "The above Example 1 is a simple example demonstrating the usage of torch.zeros_like() to initialize a tensor of zeros with the same shape as another tensor.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "x is created as a random 2x3 tensor using torch.rand().\n",
        "zeros_like() is called by passing x as the input. This creates a new tensor y with the same shape (2,3) as x.\n",
        "Since zeros_like() fills the tensor with zeros, all values in y are 0.\n",
        "Printing y displays the resulting 2x3 tensor full of zeros.\n",
        "So in just two lines of code, we can use the shape of x to initialize y as a zeros tensor using zeros_like().\n",
        "\n",
        "The output shows the tensor y populated with 0 values in the same 2x3 dimensional structure as x. This demonstrates the usefulness of zeros_like() to quickly create zero tensors matching the size of any existing tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAga_dapGeOk",
        "outputId": "f2d98741-00c0-4564-b152-e83d601dda37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0.]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "# Example 2 - working\n",
        "w = torch.empty(4, 4)\n",
        "b = torch.zeros_like(w, requires_grad=True) # b requires grad\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_IbNNz7GeOm"
      },
      "source": [
        "This example demonstrates using zeros_like() to create a tensor for holding model parameters that require gradients.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "w is created as an empty 4x4 tensor using torch.empty().\n",
        "zeros_like(w) is used to create a new tensor b with the same shape (4,4) and populated with zeros.\n",
        "We pass requires_grad=True to have gradients tracked for this tensor, since it will hold model parameters.\n",
        "Printing b shows the 4x4 tensor of zeros with requires_grad=True.\n",
        "This allows easily creating a zero initialized tensor b to hold learnable parameters, matching the shape of any existing tensor w.\n",
        "\n",
        "The output verifies b is 4x4, full of zeros, and has requires_grad set to True for gradient tracking during model training.\n",
        "\n",
        "So zeros_like() can be used to conveniently initialize parametric tensors suited for neural network training, in addition to clearing tensor values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "EGV_yZZGGeOm",
        "outputId": "5c0e873f-2e3c-40bf-e91c-e701a20f4b38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "zeros_like(): argument 'input' (position 1) must be Tensor, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-d62ef4a2352a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error, input must be tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: zeros_like(): argument 'input' (position 1) must be Tensor, not str"
          ]
        }
      ],
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "z = 'string'\n",
        "torch.zeros_like(z) # Error, input must be tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_CtUbHMGeOn"
      },
      "source": [
        "This error occurs because zeros_like() expects the input to be a PyTorch tensor, but we passed it a string 'string'.\n",
        "\n",
        "The input argument for zeros_like() must be a tensor whose shape can be replicated. A string does not have a shape attribute, hence passing it to zeros_like() results in a TypeError.\n",
        "\n",
        "To rectify this, we need to pass a proper PyTorch tensor as the input. For example:\n",
        "# Fix\n",
        "\n",
        "# import torch\n",
        "\n",
        "# x = torch.empty(3, 4) # Tensor as input\n",
        "\n",
        "# y = torch.zeros_like(x) # Works!\n",
        "\n",
        "The key steps to fix this:\n",
        "\n",
        "Import torch and create a tensor variable.\n",
        "Pass this tensor variable as the input to zeros_like() instead of a string.\n",
        "This satisfies the requirement of the input being a tensor. zeros_like() can then infer the shape and device from the input tensor to create the output zero tensor as expected.\n",
        "\n",
        "So in summary, the input to zeros_like() must always be a PyTorch tensor with shape information, not a string or other data type. This allows zeros_like() to replicate the shape and device in the new zero tensor it returns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_mzEBFyGeOo"
      },
      "source": [
        "The torch.zeros_like() function provides a simple yet effective way to initialize tensors filled with zeros. Its core use cases are:\n",
        "\n",
        "Initializing model parameters: By passing an existing tensor and setting requires_grad=True, zeros_like() allows creating trainable tensors suited for optimization. This avoids manual tensor creation code.\n",
        "\n",
        "Clearing tensor values: When the existing contents of a tensor need to be erased before further computations, zeros_like() offers a concise way to reset values.\n",
        "\n",
        "Ensuring numerical stability: Filling tensors with zeros can help avoid accidental high initializations that can impede convergence.\n",
        "\n",
        "Overall, zeros_like() removes the boilerplate of building zeros tensors manually. With its intuitive syntax torch.zeros_like(input_tensor), it makes clearing, parameter initialization and value resetting easy. For both research prototyping and production systems, zeros_like() simplifies the process of configuring tensors for learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTNn7m4HGeOo"
      },
      "source": [
        "Let's save our work using Jovian before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "H-ZNu-LKGeOp"
      },
      "outputs": [],
      "source": [
        "!pip install jovian --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "S1cT8ez8GeOp"
      },
      "outputs": [],
      "source": [
        "import jovian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "to5_cMt0GeOq",
        "outputId": "e815ac84-61f1-4dca-81fc-9a9db249c044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "Committed successfully! https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "jovian.commit(project='Introduction to Key PyTorch Tensor Functions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L27euHbPGeOq"
      },
      "source": [
        "## Function 2 - torch.normal\n",
        "\n",
        "PyTorch's torch.normal() function is an easy way to generate random tensor values from a normal or Gaussian distribution. It is commonly used for weight initialization in neural networks to break symmetry and introduce controlled randomness.\n",
        "\n",
        "The normal() function takes in the mean and standard deviation parameters that define the normal distribution. By adjusting these, various forms of random values with different ranges can be produced. The function samples values for each element in the output tensor independently from the distribution.\n",
        "\n",
        "Using normal initialization allows weights to vary across a reasonable range to start training. This helps break symmetry compared to using all zeroes. The downside is that choosing very wide distributions can sometimes be unstable. So care is needed in tuning the parameters.\n",
        "\n",
        "Overall, torch.normal() provides a simple API for introducing properly controlled randomness from a normal distribution into tensor values. This is very useful for large scale neural network initialization where correct weight ranges are critical for performance.\n",
        "\n",
        "- torch.normal(mean, std, size)\n",
        "- Creates a tensor with values sampled from the normal distribution.\n",
        "- Usage: Initialize weights in some neural network layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6DEReLaGeOr",
        "outputId": "8d80858d-001e-4816-d880-bd561b0654bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.8973,  0.3160, -0.3590],\n",
            "        [ 1.3435, -1.4594, -2.6579],\n",
            "        [ 0.6830, -0.6134, -0.7292]])\n"
          ]
        }
      ],
      "source": [
        "# Example 1 - working\n",
        "x = torch.normal(0, 1, size=(3,3)) # Normal distribution\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxqIznGDGeOr"
      },
      "source": [
        "This PyTorch code demonstrates using torch.normal() to generate a tensor with values sampled from a normal distribution.\n",
        "\n",
        "Here are the key steps:\n",
        "\n",
        "normal() is called with mean=0 and std=1. This specifies a standard normal distribution.\n",
        "The size argument indicates the desired shape of the output tensor, which is 3x3.\n",
        "normal() then samples from the N(0, 1) distribution, generating a 3x3 tensor with values populated randomly according to the normal PDF.\n",
        "Printing this tensor shows the output contains random floats scattered around 0. Most values are between -2 to 2, reflecting the shape of a standard normal distribution.\n",
        "Each element is sampled independently, so we get different random values in each position of this 3x3 tensor.\n",
        "In summary, this shows how normal() can be used to easily generate tensors with elements populated randomly using a normal distribution. By adjusting the parameters, we can control the spread of values for the use case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXba-oQfGeOr",
        "outputId": "8766b1fb-1e84-41ba-b7b8-78cd5e90a5fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.2817, -0.0411,  0.0580, -0.0309,  0.1177])\n"
          ]
        }
      ],
      "source": [
        "# Example 2 - working\n",
        "y = torch.normal(0, 0.1, size=(5,)) # Small std dev\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo-33UKDGeOs"
      },
      "source": [
        "This example demonstrates using torch.normal() to generate values from a narrow normal distribution.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "normal() is called with mean 0 and std of 0.1. This specifies a tight normal distribution.\n",
        "The size argument creates a tensor with 5 elements.\n",
        "normal() samples from N(0, 0.1) and populates the 5 element tensor.\n",
        "Printing shows the output values are clustered tightly around 0, with no values outside [-0.2, 0.2].\n",
        "This narrow range reflects the low std deviation of 0.1 specified.\n",
        "In essence, this shows that reducing the std dev parameter to normal() results in generating values concentrated in a narrow band around the mean.\n",
        "\n",
        "Setting a low std dev is useful when you need controlled variation but also want to prevent wide ranges in the initial values. This demonstrates how normal() provides fine grained control over the distribution to suit different initialization needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "x7lUCIpiGeOs",
        "outputId": "b9812b58-a514-4fa1-9cc1-32383b9bcdf2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "normal() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (Tensor mean, float std, *, torch.Generator generator, Tensor out)\n * (float mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (float mean, float std, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-bb577c92d781>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error, missing size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: normal() received an invalid combination of arguments - got (int, int), but expected one of:\n * (Tensor mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (Tensor mean, float std, *, torch.Generator generator, Tensor out)\n * (float mean, Tensor std, *, torch.Generator generator, Tensor out)\n * (float mean, float std, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
          ]
        }
      ],
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "z = torch.normal(0, 1) # Error, missing size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRi8hHdXGeOt"
      },
      "source": [
        "This error occurs because we did not specify the size argument when calling torch.normal().\n",
        "\n",
        "The normal() function requires either:\n",
        "\n",
        "Input tensors for mean and std\n",
        "Scalar values for mean and std along with a size\n",
        "In this example, we passed scalar mean and std values, but did not provide the output tensor size.\n",
        "\n",
        "To fix this, we need to add the size argument:\n",
        "# Fix\n",
        "\n",
        "# z = torch.normal(0, 1, size=(3,3)) # Pass size\n",
        "\n",
        "This provides the shape for the output tensor that normal() should generate.\n",
        "\n",
        "The key steps to rectify this:\n",
        "\n",
        "Determine if passing tensor or scalar inputs for mean, std\n",
        "If scalar inputs, must include size to specify output shape\n",
        "Pass size as a tuple, e.g. (3,3) or (5,)\n",
        "So in summary, when using scalar inputs for mean and std dev, the size argument is mandatory to tell normal() what shape of tensor to generate. Adding this size parameter will resolve the error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO1bhdDgGeOt"
      },
      "source": [
        "Here are some closing comments on when torch.normal() is useful:\n",
        "\n",
        "Weight initialization in neural networks - It provides randomized starting values that break symmetry and enable effective training. Normal distributions are commonly used.\n",
        "Introducing randomness into models - The controlled variation from normal samples can help improve generalization capability.\n",
        "Data augmentation - Adding normally distributed noise to training data can act as regularization.\n",
        "Generating test inputs - Sampling random inputs from a distribution like normal can help evaluate model robustness.\n",
        "Monte Carlo simulations - Normal sampling allows estimating quantities that are hard to compute analytically.\n",
        "In summary, torch.normal() is a simple building block that enables controlled normal random number generation. This drives several use cases in deep learning and probabilistic modeling where normal distributions are useful mathematical tools. The ease-of-use makes it ideal for quickly introducing normal random data into tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "XmtHBa-1GeOt",
        "outputId": "6fb4645c-3374-431e-b5b9-b9664c34ff67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "Committed successfully! https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "jovian.commit(project='Introduction to Key PyTorch Tensor Functions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_2gcMXqGeOu"
      },
      "source": [
        "## Function 3 - torch.hstack\n",
        "\n",
        "PyTorch's torch.hstack() function concatenates sequence of tensors along a new horizontal axis. It allows joining multiple tensors to create a single combined tensor by stacking them horizontally.\n",
        "\n",
        "The hstack() function takes a sequence of input tensors and concatenates them next to each other in order to form a new tensor with an additional axis. This is useful for creating batches of training data, combining multi-channel images, or merging features from different sources.\n",
        "\n",
        "The main advantage of hstack() is that it provides a quick and convenient way to assemble data for models that expect a certain tensor shape or feature combination. However, it does involve creating a copy of the data into a new tensor, which can have performance implications. Also, care must be taken to ensure the non-concatenation dimensions match correctly.\n",
        "\n",
        "In conclusion, torch.hstack() is a handy tool for combining collections of tensors in an intuitive way along a new horizontal axis. The simplicity it offers makes it very useful across a wide range of deep learning workflows dealing with sequenced or multi-view data.\n",
        "\n",
        "- torch.hstack(tensors)\n",
        "- Stacks tensors horizontally.\n",
        "- Usage: Concatenate tensors along a new horizontal axis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VexYUwOLGeOv",
        "outputId": "3f6e7982-aaac-47b7-f814-a8a751738c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.4527, -1.2367,  0.4981, -0.3102,  0.3514],\n",
            "        [ 0.1374,  0.2587, -0.1445, -1.3714,  0.2838]])\n"
          ]
        }
      ],
      "source": [
        "# Example 1 - working\n",
        "x = torch.randn(2, 3)\n",
        "y = torch.randn(2, 2)\n",
        "z = torch.hstack([x, y]) # z is 2x5\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy88cwrTGeOv"
      },
      "source": [
        "This example demonstrates using torch.hstack() to concatenate two tensors horizontally.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "x and y are created as random 2x3 and 2x2 tensors respectively using torch.randn().\n",
        "hstack() is called by passing a list containing x and y.\n",
        "This concatenates x and y next to each other horizontally.\n",
        "The resulting tensor z has shape 2x5 - the height remains 2 from x and y, while the width sums to 3 + 2 = 5.\n",
        "Printing z displays the concatenated 2x5 tensor with x's values on the left and y's values on the right.\n",
        "In summary, this shows how hstack() can be used to combine two tensors along a new horizontal axis easily. The tensors just need to have the same height dimension - the widths can differ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eVNHWExGeOw",
        "outputId": "12017b5b-aed4-4036-eed2-00ed42e43051"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Example 2 - working\n",
        "a = torch.tensor([1,2,3])\n",
        "b = torch.tensor([4,5,6])\n",
        "torch.hstack([a,b]) # [1,2,3,4,5,6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv8nrlnwGeOw"
      },
      "source": [
        "his example shows the use of torch.hstack() to concatenate 1D tensors.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "Tensors a and b are created with shapes 3 and 3 respectively using torch.tensor().\n",
        "hstack() is called by passing a list containing a and b.\n",
        "As a and b are 1D, hstack concatenates them along a new axis 0, stacking them horizontally.\n",
        "The resulting tensor is 1D with shape 6, containing the values from a followed by values from b.\n",
        "Printing this tensor displays [1, 2, 3, 4, 5, 6] - the elements of a and b concatenated.\n",
        "So this demonstrates that hstack() can work on 1D tensors as well, joining them along a new 0th dimension.\n",
        "\n",
        "The key thing is that the non-concatenation dimensions (originally 0) must match. Here both a and b had size 3, so they could be horizontally stacked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "4zmX2JgFGeOw",
        "outputId": "bf658114-2c88-45d9-cbed-4e4dd3227c43"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "hstack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-cb5f25063924>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error, expected list of tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: hstack(): argument 'tensors' (position 1) must be tuple of Tensors, not Tensor"
          ]
        }
      ],
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "p = torch.tensor([1,2])\n",
        "torch.hstack(p) # Error, expected list of tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kaSsAbDGeOw"
      },
      "source": [
        "This PyTorch code is attempting to use the torch.hstack() function to horizontally stack tensors, but is passing an invalid argument which causes an error.\n",
        "\n",
        "Specifically:\n",
        "\n",
        "p is defined as a PyTorch tensor with values [1,2]\n",
        "torch.hstack() expects a tuple/list of tensors as the first argument, but p is a single tensor\n",
        "So when torch.hstack(p) is called, it raises a TypeError saying it expected a tuple of Tensors, not a single Tensor\n",
        "To fix this, we need to pass a tuple/list of tensors to torch.hstack(). For example:\n",
        "\n",
        "# p1 = torch.tensor([1,2])\n",
        "# p2 = torch.tensor([3,4])\n",
        "\n",
        "# torch.hstack((p1, p2)) # Pass a tuple of tensors\n",
        "\n",
        "This will horizontally stack the two tensors p1 and p2, with no error.\n",
        "\n",
        "In summary, the key points are:\n",
        "\n",
        "torch.hstack() expects a tuple/list of tensors as the first argument\n",
        "But a single tensor p was passed instead of a tuple\n",
        "This caused a TypeError saying a tuple of tensors is expected\n",
        "To fix it, need to pass a tuple of tensors rather than a single tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q04widTGeOx"
      },
      "source": [
        "The torch.hstack() function in PyTorch is useful when you need to horizontally stack a sequence of tensors together. This allows you to combine multiple tensors side-by-side to create a larger tensor.\n",
        "\n",
        "Some common use cases where torch.hstack() would be applicable:\n",
        "\n",
        "Combining feature vectors from different sources - For example, in NLP you may have word embeddings from different models and want to concatenate them to create a combined embedding.\n",
        "Stacking frames in sequence data - For video or audio data, you may have a sequence of frames/samples that you want to join together into a larger tensor representing the full sequence.\n",
        "Combining batches - You may have data from multiple mini-batches and want to concatenate them into a larger batch for certain operations.\n",
        "Expanding tensor dimensions - Joining tensors using hstack can increase the dimensions of the data (e.g. combining row vectors to create a matrix).\n",
        "So in summary, torch.hstack() provides a convenient way to join tensors along their horizontal axis, enabling various use cases where you need to combine multiple tensors together. It's a useful tool for preprocessing and transforming tensor data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdlDtH4CGeOy"
      },
      "source": [
        "## Function 4 - torch.sqrt\n",
        "\n",
        "The torch.sqrt() function in PyTorch is used to calculate the square root of each element in the input tensor.\n",
        "\n",
        "It takes a single tensor as input and applies the square root operation element-wise, returning a new tensor with the square roots. Some key properties:\n",
        "\n",
        "The input tensor can be of any shape. The output will have the same shape.\n",
        "The input tensor should contain only non-negative real numbers, as square roots of negative numbers are complex.\n",
        "The function is applied element-wise. Each element in the input tensor gets squared independently.\n",
        "The input and output are both PyTorch tensors.\n",
        "The square root values are calculated in floating point precision, even if the input tensor has integer types.\n",
        "\n",
        "- torch.sqrt(input)\n",
        "- Computes the square root of each element of the input tensor.\n",
        "- Usage: Useful for normalization and scaling.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YavEX8DRGeOy",
        "outputId": "a927ddb0-721e-4923-f59e-a6b0e87751aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 3., 4.])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Example 1 - working\n",
        "x = torch.tensor([4, 9, 16])\n",
        "torch.sqrt(x) # [2, 3, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KeLPkJpGeOy"
      },
      "source": [
        "To summarize:\n",
        "\n",
        " - x is a 1D tensor with values [4, 9, 16]\n",
        " - torch.sqrt() calculates element-wise square root\n",
        " - The output tensor contains the square roots [2, 3, 4]\n",
        "\n",
        "So torch.sqrt() can be used to conveniently calculate element-wise square roots on tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MeADtQaGeOy",
        "outputId": "9d3e7de4-7e9b-4c52-ee29-45030c721017"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4088, 0.8912, 0.7630],\n",
              "        [0.5382, 0.8971, 0.8261]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Example 2 - working\n",
        "y = torch.rand(2,3)\n",
        "torch.sqrt(y) # Square root of each element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eE6YpT6GeOz"
      },
      "source": [
        "The torch.sqrt() function in PyTorch calculates the element-wise square root of the input tensor. It takes a tensor of any shape as input, applies the square root operation on each element independently, and returns a tensor of the same shape with the computed square roots. This makes it easy to obtain the square root of each element in tensors of any dimension without having to use explicit loops or indexing. torch.sqrt() supports tensors containing non-negative real values as input, since it computes the square root using floating point precision. In just one line of code, it can compute element-wise square roots on multidimensional tensor data for tasks requiring squared roots of tensor elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "fuy513OzGeO8",
        "outputId": "5f049aa5-4532-40e2-d6ea-d43ccea03d6a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "sqrt(): argument 'input' (position 1) must be Tensor, not str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-371484307753>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error, expected tensor input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: sqrt(): argument 'input' (position 1) must be Tensor, not str"
          ]
        }
      ],
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "z = 'string'\n",
        "torch.sqrt(z) # Error, expected tensor input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMQ7_MhcGeO9"
      },
      "source": [
        "The code in Example 3 results in an error because torch.sqrt() expects a PyTorch tensor as input, but we pass a string 'string' instead.\n",
        "\n",
        "It gives a TypeError saying:\n",
        "\n",
        "\"argument 'input' (position 1) must be Tensor, not str\"\n",
        "\n",
        "This indicates torch.sqrt() got an invalid input type - it requires a Tensor but received a string.\n",
        "\n",
        "To fix this, we need to pass a valid PyTorch tensor as the input instead of a string.\n",
        "\n",
        "For example:\n",
        "\n",
        "# import torch\n",
        "\n",
        "# z = torch.tensor([4., 9., 16.])\n",
        "# torch.sqrt(z)\n",
        "\n",
        "Now z is a tensor containing numeric values. Torch.sqrt() will apply element-wise square root successfully.\n",
        "\n",
        "So the key points are:\n",
        "\n",
        "torch.sqrt() expects a PyTorch Tensor as input\n",
        "But we passed a string instead of a tensor\n",
        "This caused a TypeError saying it expected a Tensor\n",
        "To fix, we need to pass a proper tensor as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kUnkHueGeO9"
      },
      "source": [
        "The torch.sqrt() function provides a simple and convenient way to calculate element-wise square roots of tensor data in PyTorch. Here are some closing notes on when it can be useful:\n",
        "\n",
        "Computing distances - Element-wise square roots are needed for distance metrics like Euclidean distance. Torch.sqrt() can efficiently compute this.\n",
        "Neural network activations - Non-linear activations like softmax involve computing square roots, which can be done with torch.sqrt().\n",
        "Un-normalizing data - If data was normalized by squaring, torch.sqrt() can reverse this.\n",
        "Statistical operations - Square roots are useful for stats like standard deviation. Torch.sqrt() enables this on tensor data.\n",
        "Preprocessing - It can be used as part of preprocessing pipelines to transform features like taking square roots.\n",
        "Hardware acceleration - Torch.sqrt() will utilize GPUs for accelerated computation if tensors are on GPU.\n",
        "In summary, torch.sqrt() is a simple building block that can be applied in various contexts where computing element-wise square roots on tensor data is needed. It makes basic mathematical operations easy and fast."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrh7WSHJGeO-"
      },
      "source": [
        "## Function 5 - torch.gather\n",
        "\n",
        "torch.gather() is a function in PyTorch that gathers values from a tensor along a specified axis according to given indices.\n",
        "\n",
        "It allows selecting particular elements from a tensor based on a set of indices. The gather() function takes an input tensor, a dimension number, and an index tensor containing indices to gather along that dimension. It then outputs a new tensor by collecting the elements from the input tensor corresponding to the given indices.\n",
        "\n",
        "For example, given a 3D tensor, gather() could be used to extract rows or columns by gathering along a particular axis based on row/column indices specified in the index tensor. This provides an efficient way to index into tensors to select subsets of elements.\n",
        "\n",
        "The gather() function is typically used for slicing tensors, filtering tensor values, fetching rows/columns from matrices, and retrieving other selective slices from multi-dimensional data. It provides fine-grained control over accessing and manipulating parts of tensors based on numeric indices.\n",
        "\n",
        "Overall, torch.gather() is a powerful indexing function for slicing tensors and extracting selective values in PyTorch.\n",
        "\n",
        "- torch.gather(input, dim, index)\n",
        "- Gathers values along an axis specified by dim.\n",
        "- Usage: Select/filter out elements based on index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky-PxsJ1GeO_",
        "outputId": "33df02ef-bf48-482e-92eb-7e0b09e96804"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1],\n",
              "        [4, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Example 1 - working\n",
        "x = torch.tensor([[1,2],[3,4]])\n",
        "torch.gather(x, 1, torch.tensor([[0,0],[1,0]])) # [[1,1],[4,3]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sqlLyztGePA"
      },
      "source": [
        "This example demonstrates using torch.gather() to gather elements from a 2D tensor based on indices.\n",
        "\n",
        "The key steps are:\n",
        "\n",
        "Tensor x is created with shape 2x2 containing values 1,2,3,4.\n",
        "gather() is called on x, with dimension index 1 i.e we want to gather along axis 1.\n",
        "The index tensor passed is 2x2, containing indices 0 and 1.\n",
        "This gathers 0th and 1st values along each row of x based on the indices.\n",
        "Result is a 2x2 tensor where each row contains x's 0th and 1st columns gathered based on indices.\n",
        "In summary, this shows how gather can be used along a particular dimension to selectively pick tensor values based on an index tensor. The output contains elements gathered from x according to the indices in each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLQwRN5wGePA",
        "outputId": "5c9b499d-47bc-4165-a472-7fbf09dfef1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5479],\n",
              "        [0.0370]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# Example 2 - working\n",
        "y = torch.rand(3,4)\n",
        "idx = torch.tensor([[0], [1]]) # idx is now 2D\n",
        "\n",
        "torch.gather(y, 0, idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPO7HIEwGePA"
      },
      "source": [
        "Here is an explanation of the code:\n",
        "\n",
        " - A 3x4 tensor y is created using torch.rand(), with random values.\n",
        " - An index tensor idx is created with shape 2x1, containing indices 0 and 1.\n",
        " - torch.gather() is called on tensor y, along dimension 0 (rows).\n",
        " - idx contains the indices [0, 1] to select from the rows.\n",
        " - gather() gathers the 0th and 1st rows of y based on idx.\n",
        " - The output is a 2x1 tensor containing the gathered 0th and 1st rows.\n",
        "\n",
        "In summary, this gathers the first two rows of the 3x4 tensor y, by selecting rows along dimension 0 based on the indices in idx.\n",
        "\n",
        "The key thing is idx must have the same number of dimensions as y, to index correctly. The output contains the rows of y that correspond to the indices in idx."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "C7eIvnZVGePB",
        "outputId": "54a0bd7b-a79f-44b4-de9a-a1adc86e767d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-593004047736>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking (to illustrate when it breaks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error, dim 2 does not exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ],
      "source": [
        "# Example 3 - breaking (to illustrate when it breaks)\n",
        "z = torch.gather(x, 2, idx) # Error, dim 2 does not exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkLhmLK6GePB"
      },
      "source": [
        "Here is an explanation of the code and how to fix the error:\n",
        "\n",
        "The code is trying to gather values from a tensor x along dimension 2. However, x is a 2D tensor as defined previously, so it only has dimensions 0 and 1.\n",
        "\n",
        "When calling torch.gather(), the dimension index passed in must be a valid axis for that tensor. Since x is 2D, only -2, -1, 0 or 1 are valid.\n",
        "\n",
        "By passing 2 as the dim index, it is out of range for the dimensions of x. Hence PyTorch throws an IndexError noting that 2 is outside the valid range.\n",
        "\n",
        "To fix this, the dim index needs to be changed to 0 or 1 to gather along the rows or columns of x:\n",
        "\n",
        "# Fix\n",
        "\n",
        "# z = torch.gather(x, 0, idx) # Gather along rows (dim 0)\n",
        "\n",
        "\n",
        "The key things to remember:\n",
        "\n",
        "Check the tensor dimensions\n",
        "Pass a valid dim index to gather() based on tensor shape\n",
        "Use -1, 0 etc to refer to last, first dimensions etc.\n",
        "By choosing a in-range dimension, the gather() call will work correctly without the index error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNyLdPMPGePB"
      },
      "source": [
        "Here are some closing comments on when torch.gather() is useful:\n",
        "\n",
        "Slicing tensors - Gathering along a dimension using indices allows selectively extracting subsets of values. This provides fine-grained tensor slicing.\n",
        "Implementing sparse layers - Gather can efficiently fetch outputs of previous layers based on sparse connectivity patterns.\n",
        "Building segmentation models - For tasks like image segmentation, gather can help fetch pixel locations and group them.\n",
        "Retrieving rows/columns from matrices - Passing 1D indices allows neatly picking rows or columns from 2D matrices.\n",
        "Filtering tensor values - Gather acts as an efficient filter to cherry pick elements based on conditions encoded in the indices.\n",
        "In summary, torch.gather() provides indexing capabilities to selectively retrieve data from tensors. The ability to pick elements based on custom indices enables building a variety of neural network layers and models for computer vision, NLP and other domains. It's a powerful building block for manipulating tensor data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "uZRq9wVfGePC",
        "outputId": "6f49d5ea-e26b-4896-d80c-6906c7cc4ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "Committed successfully! https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "jovian.commit(project='Introduction to Key PyTorch Tensor Functions')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuSJQwHoGePC"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Here is a summary of the key points covered in this notebook:\n",
        "\n",
        " 1. torch.zeros_like() - Initializes a tensor with zeros based on shape of input tensor. Useful for clearing tensor values and parameter initialization.\n",
        " 2. torch.normal() - Generates random numbers from a normal distribution. Commonly used for weight initialization in neural networks.\n",
        " 3. torch.hstack() - Stacks sequence of tensors horizontally by concatenating along a new axis. Helps combine data for models.\n",
        " 4. torch.gather() - Gathers values from a tensor along a given dimension based on index tensor. Enables slicing and filtering tensors.\n",
        " 5. torch.sqrt() - Calculates element-wise square root of input tensor. Useful for normalization and scaling.\n",
        "\n",
        "Through examples, we explored how these functions help with initialization, random data generation, combining tensors, indexing and math operations.\n",
        "\n",
        "Some suggestions for next steps:\n",
        "\n",
        " 1. Experiment with these functions by applying them on sample data for your models\n",
        " 2. Explore other PyTorch tensor functions like stack, cat, masked_select etc.\n",
        " 3. Learn about PyTorch modules like nn, optim for building neural networks\n",
        " 4. Work through PyTorch autograd, loss functions and optimization\n",
        " 5. Build a small end-to-end model with PyTorch using the covered concepts\n",
        "\n",
        "The tensor functions provide the basic building blocks. Next it would be useful to learn how PyTorch supports building and training neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CtDHdAjGePD"
      },
      "source": [
        "## Reference Links\n",
        "Provide links to your references and other interesting articles about tensors\n",
        "* Official documentation for tensor operations: https://pytorch.org/docs/stable/torch.html\n",
        "* https://pytorch.org/docs/stable/generated/torch.zeros_like.html\n",
        "* https://pytorch.org/docs/stable/generated/torch.normal.html\n",
        "* https://pytorch.org/docs/stable/generated/torch.hstack.html\n",
        "* https://pytorch.org/docs/stable/generated/torch.sqrt.html\n",
        "* https://pytorch.org/docs/stable/generated/torch.gather.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ndfimc5eGePD",
        "outputId": "a8ff3bd8-d68d-4387-f2d3-18e88176a654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[jovian] Detected Colab notebook...\u001b[0m\n",
            "[jovian] Uploading colab notebook to Jovian...\u001b[0m\n",
            "Committed successfully! https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://jovian.com/willymwa85/introduction-to-key-pytorch-tensor-functions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "jovian.commit(project='Introduction to Key PyTorch Tensor Functions')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWK-kIibGePD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}